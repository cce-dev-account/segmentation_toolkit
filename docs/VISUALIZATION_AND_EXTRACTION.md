# Cluster Visualization and Segment Extraction

This document describes the new capabilities for visualizing segmentation clusters and extracting segment sub-samples from your IRB segmentation framework.

## Overview

Two new tools have been added to the framework:

1. **`scripts/visualize_clusters.py`**: Visualize segmentation clusters in 2D using dimensionality reduction
2. **`scripts/extract_segments.py`**: Extract sub-samples of data by segment ID

Both tools work with the JSON outputs generated by the segmentation pipeline (`baseline_report.json` and `tree_structure.json`).

---

## 1. Cluster Visualization

### Purpose

Visualize how your segmentation divides the feature space:
- **Scatter plots** showing clusters in 2D (PCA, t-SNE, UMAP)
- **Heatmaps** showing default rate gradients
- **Feature distributions** across segments
- Color-coded by risk level (default rate)

### Quick Start

```python
from scripts.visualize_clusters import ClusterVisualizer

# After running your segmentation pipeline
viz = ClusterVisualizer('output/baseline_report.json')

# Load data (from pipeline)
viz.load_data_from_arrays(
    X_train=pipeline.X_train,
    y_train=pipeline.y_train,
    segments_train=pipeline.baseline_engine.segments_train_,
    feature_names=pipeline.feature_names
)

# Reduce dimensions
viz.reduce_dimensions('pca')

# Create visualizations
viz.plot_clusters(save_path='output/clusters_pca.png')
viz.plot_default_rate_heatmap(save_path='output/heatmap.png')
viz.plot_feature_distributions(save_path='output/features.png')
```

### Dimensionality Reduction Methods

**PCA (Principal Component Analysis)**
- Fast, linear method
- Good for understanding overall variance
- Shows explained variance ratios

```python
viz.reduce_dimensions('pca')
```

**t-SNE (t-Distributed Stochastic Neighbor Embedding)**
- Non-linear, preserves local structure
- Better separation between clusters
- Slower (recommended for < 50k observations)

```python
viz.reduce_dimensions('tsne', perplexity=30)
```

**UMAP (Uniform Manifold Approximation and Projection)**
- Non-linear, preserves both local and global structure
- Fast and scalable
- Requires `pip install umap-learn`

```python
viz.reduce_dimensions('umap')
```

### Visualization Types

#### 1. Scatter Plot

Shows clusters colored by segment, with optional centroids and statistics:

```python
fig, ax = viz.plot_clusters(
    title="Segmentation Clusters (PCA)",
    show_centroids=True,
    show_stats=True,
    save_path='clusters.png'
)
```

**Features:**
- Color-coded by default rate (green=low risk, red=high risk)
- Segment centroids marked with stars
- Legend with segment PD rates
- Statistics box with overall metrics

#### 2. Default Rate Heatmap

Shows gradient of default rates across the 2D space:

```python
fig, ax = viz.plot_default_rate_heatmap(
    resolution=100,  # Grid resolution
    save_path='heatmap.png'
)
```

**Features:**
- Smooth heatmap using k-nearest neighbors
- Overlay of individual observations
- Red/yellow/green color scheme

#### 3. Feature Distributions

Shows how features vary across segments:

```python
fig, axes = viz.plot_feature_distributions(
    features=['credit_score', 'dti', 'interest_rate'],  # Specific features
    # Or automatically select top features by variance
    n_features=6,
    save_path='features.png'
)
```

### Generate All Visualizations

Create all visualization types at once:

```python
viz.create_all_visualizations(output_prefix='output/viz')
```

Generates:
- `output/viz_pca_scatter.png`
- `output/viz_pca_heatmap.png`
- `output/viz_tsne_scatter.png` (if dataset < 50k)
- `output/viz_feature_distributions.png`

### Full Example

```python
from irb_segmentation import SegmentationPipeline
from irb_segmentation.config import SegmentationConfig
from scripts.visualize_clusters import ClusterVisualizer

# 1. Run segmentation
config = SegmentationConfig.from_yaml('config_examples/lending_club_categorical.yaml')
pipeline = SegmentationPipeline(config)
pipeline.load_data()
pipeline.fit_baseline()

# 2. Visualize
viz = ClusterVisualizer('output/lending_club_categorical/baseline_report.json')
viz.load_data_from_arrays(
    X_train=pipeline.X_train,
    y_train=pipeline.y_train,
    segments_train=pipeline.baseline_engine.segments_train_,
    feature_names=pipeline.feature_names
)

# 3. Generate all visualizations
viz.create_all_visualizations(output_prefix='output/lending_club_categorical/viz')
```

---

## 2. Segment Extraction

### Purpose

Extract sub-samples of your original dataset by segment ID:
- Filter observations belonging to specific segments
- Split dataset into separate files by segment
- Filter by segment criteria (default rate, density, etc.)
- Works directly from `tree_structure.json` - no need to re-run segmentation

### Quick Start

```python
from scripts.extract_segments import SegmentExtractor

# Initialize with tree structure
extractor = SegmentExtractor('output/lending_club_categorical/tree_structure.json')

# Extract low-risk segments (1 and 2)
df_low_risk = extractor.extract_from_csv(
    csv_path='data/lending_club.csv',
    segment_ids=[1, 2],
    output_path='output/low_risk_subset.csv'
)
```

### Methods

#### 1. Extract Specific Segments

Extract observations from one or more segments:

```python
# Single segment
df_seg2 = extractor.extract_from_csv(
    csv_path='data/lending_club.csv',
    segment_ids=2,
    output_path='segment_2.csv'
)

# Multiple segments
df_high_risk = extractor.extract_from_csv(
    csv_path='data/lending_club.csv',
    segment_ids=[4, 5],  # High risk segments
    output_path='high_risk.csv'
)
```

#### 2. Split by All Segments

Create separate CSV files for each segment:

```python
output_files = extractor.split_by_segments(
    csv_path='data/lending_club.csv',
    output_dir='output/segments',
    prefix='segment'
)

# Creates:
# output/segments/segment_0.csv
# output/segments/segment_1.csv
# output/segments/segment_2.csv
# ...
```

#### 3. Filter by Criteria

Extract segments matching specific criteria:

```python
# Medium risk segments (10-20% default rate)
df_medium = extractor.filter_by_segment_criteria(
    csv_path='data/lending_club.csv',
    criteria={
        'min_default_rate': 0.10,
        'max_default_rate': 0.20
    },
    output_path='medium_risk.csv'
)

# Large, stable segments
df_stable = extractor.filter_by_segment_criteria(
    csv_path='data/lending_club.csv',
    criteria={
        'min_observations': 5000,
        'min_density': 0.10
    },
    output_path='stable_segments.csv'
)
```

**Available criteria:**
- `min_default_rate` / `max_default_rate`
- `min_density` / `max_density`
- `min_observations` / `max_observations`

#### 4. Get Segment Statistics

View statistics for all segments in your data:

```python
import pandas as pd

df = pd.read_csv('data/lending_club.csv')
X = df[extractor.feature_names].fillna(0).values
y = df['default'].values

stats = extractor.get_segment_statistics(X, y)
print(stats)
```

Output:
```
   segment_id  n_observations  density  n_defaults  default_rate
0           1          21376   0.2229       1404        0.0657
1           2          27380   0.2855       4561        0.1666
2           3          22398   0.2336       2519        0.1125
3           4          13814   0.1440       4117        0.2980
```

#### 5. Work with NumPy Arrays

Extract from arrays instead of CSV:

```python
X_filtered, y_filtered = extractor.extract_from_arrays(
    X=X_train,
    y=y_train,
    segment_ids=[1, 2]
)

# Or get indices
result = extractor.extract_from_arrays(
    X=X_train,
    y=y_train,
    segment_ids=[1, 2],
    return_indices=True
)
# result = {'indices': [...], 'X': [...], 'y': [...], 'segments': [...]}
```

### Command-Line Usage

All functionality is available via command-line:

```bash
# Extract single segment
python scripts/extract_segments.py data.csv tree_structure.json \
    --segments 2 --output segment_2.csv

# Extract multiple segments
python scripts/extract_segments.py data.csv tree_structure.json \
    --segments 1 2 3 --output low_risk.csv

# Split into separate files
python scripts/extract_segments.py data.csv tree_structure.json \
    --split-all --output segment_splits/

# Show statistics
python scripts/extract_segments.py data.csv tree_structure.json --stats

# Filter by criteria
python scripts/extract_segments.py data.csv tree_structure.json \
    --min-dr 0.10 --max-dr 0.20 --output medium_risk.csv
```

### How It Works

The extractor reconstructs segment assignments by:

1. Loading the decision tree structure from `tree_structure.json`
2. Traversing the tree for each observation based on feature values
3. Using the `leaf_to_segment` mapping to assign final segments
4. Filtering observations by segment ID

This means you can assign segments to **any dataset** with matching features, not just the training data!

---

## Complete Workflow Example

```python
from irb_segmentation import SegmentationPipeline
from irb_segmentation.config import SegmentationConfig
from scripts.visualize_clusters import ClusterVisualizer
from scripts.extract_segments import SegmentExtractor

# ============================================================================
# STEP 1: Run Segmentation
# ============================================================================
config = SegmentationConfig.from_yaml('config_examples/lending_club_categorical.yaml')
pipeline = SegmentationPipeline(config)
pipeline.run_all(pause_for_edits=False)

# ============================================================================
# STEP 2: Visualize Clusters
# ============================================================================
viz = ClusterVisualizer('output/lending_club_categorical/baseline_report.json')
viz.load_data_from_arrays(
    X_train=pipeline.X_train,
    y_train=pipeline.y_train,
    segments_train=pipeline.baseline_engine.segments_train_,
    feature_names=pipeline.feature_names
)

# Generate all visualizations
viz.create_all_visualizations(
    output_prefix='output/lending_club_categorical/viz'
)

# ============================================================================
# STEP 3: Extract Segments
# ============================================================================
extractor = SegmentExtractor('output/lending_club_categorical/tree_structure.json')

# Extract low-risk portfolio (segments 1, 2)
df_low_risk = extractor.extract_from_csv(
    csv_path='data/lending_club.csv',
    segment_ids=[1, 2],
    output_path='output/low_risk_portfolio.csv'
)

# Extract high-risk portfolio (segment 4)
df_high_risk = extractor.extract_from_csv(
    csv_path='data/lending_club.csv',
    segment_ids=[4],
    output_path='output/high_risk_portfolio.csv'
)

# Split all segments for detailed analysis
output_files = extractor.split_by_segments(
    csv_path='data/lending_club.csv',
    output_dir='output/segments',
    prefix='segment'
)

print(f"Created {len(output_files)} segment files")
```

---

## Use Cases

### Portfolio Analysis
Extract different risk segments for separate modeling or reporting:
```python
# Investment grade
df_investment = extractor.extract_from_csv(data, [1, 2])

# Sub-investment grade
df_subinvestment = extractor.extract_from_csv(data, [3, 4])
```

### Model Development
Train segment-specific models on sub-samples:
```python
for seg_id in range(n_segments):
    df_seg = extractor.extract_from_csv(data, seg_id)
    # Train segment-specific model
    # Evaluate performance
```

### Quality Assurance
Visualize and validate segment separation:
```python
viz.reduce_dimensions('pca')
viz.plot_clusters(show_centroids=True)
viz.plot_default_rate_heatmap()
```

### Reporting
Generate segment-specific reports:
```python
output_files = extractor.split_by_segments(data, 'reports/')
for seg_id, path in output_files.items():
    df = pd.read_csv(path)
    # Generate segment report
```

---

## Requirements

### Basic Requirements (already installed)
- `numpy >= 1.20.0`
- `pandas >= 1.3.0`
- `scikit-learn >= 1.0.0`
- `matplotlib >= 3.3.0`
- `seaborn >= 0.11.0`

### Optional (for advanced visualization)
```bash
pip install umap-learn  # For UMAP dimensionality reduction
```

---

## Testing

Run the test script to verify functionality:

```bash
python test_new_features.py
```

This will:
1. Import both modules
2. Load existing outputs (if available)
3. Test basic functionality
4. Display segment statistics

For a comprehensive demo:

```bash
python examples/demo_visualization_and_extraction.py
```

---

## Files Created

- **`scripts/visualize_clusters.py`**: Cluster visualization tool (570 lines)
- **`scripts/extract_segments.py`**: Segment extraction tool (520 lines)
- **`examples/demo_visualization_and_extraction.py`**: Usage examples (300 lines)
- **`test_new_features.py`**: Quick functionality test (80 lines)
- **`docs/VISUALIZATION_AND_EXTRACTION.md`**: This documentation

---

## Troubleshooting

### "Data not loaded" error in visualizer
The visualizer needs data arrays to work. You must either:
1. Use `viz.load_data_from_arrays()` with data from the pipeline
2. Use `viz.load_data_from_csv()` with a CSV file (requires tree_structure.json)

### "Feature count mismatch" in extractor
Ensure your CSV has the same features used during segmentation. Check:
```python
print(extractor.feature_names)
```

### "Tree structure not found"
Run the segmentation pipeline first to generate `tree_structure.json`:
```python
pipeline.run_all()
```

### Visualization looks crowded
- Try different dimensionality reduction methods (t-SNE, UMAP)
- Adjust figure size: `viz.plot_clusters(figsize=(15, 10))`
- Filter by criteria to show fewer segments

---

## API Reference

### ClusterVisualizer

**Constructor:**
```python
ClusterVisualizer(baseline_report_path, data_path=None)
```

**Key Methods:**
- `load_data_from_arrays(X_train, y_train, segments_train, feature_names, ...)`
- `load_data_from_csv(csv_path, target_col='default')`
- `reduce_dimensions(method='pca', n_components=2, **kwargs)`
- `plot_clusters(title=None, figsize=(12,8), save_path=None, ...)`
- `plot_default_rate_heatmap(figsize=(12,8), save_path=None, resolution=100)`
- `plot_feature_distributions(features=None, n_features=6, ...)`
- `create_all_visualizations(output_prefix=None)`

### SegmentExtractor

**Constructor:**
```python
SegmentExtractor(tree_path)
```

**Key Methods:**
- `assign_segments(X)` â†’ segments array
- `extract_from_csv(csv_path, segment_ids, output_path=None, ...)`
- `extract_from_dataframe(df, segment_ids, feature_columns=None)`
- `extract_from_arrays(X, segment_ids, y=None, return_indices=False)`
- `split_by_segments(csv_path, output_dir, prefix='segment')`
- `filter_by_segment_criteria(csv_path, criteria, output_path=None)`
- `get_segment_statistics(X, y=None)`

---

## Next Steps

1. Run your segmentation pipeline to generate outputs
2. Test the visualization tool with your data
3. Extract segments for further analysis
4. Integrate into your workflow or reporting pipeline

For questions or issues, refer to the main README or the inline code documentation.
